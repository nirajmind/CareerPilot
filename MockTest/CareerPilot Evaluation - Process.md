‚≠ê 2. How to Track Test Cases, Scores, and Improvements
You need a structured way to:
- Upload a resume
- Upload a JD
- Capture Gemini‚Äôs output
- Score the FitGraph
- Track skill gaps
- Track mock interview performance
- Track resume rewrite quality
- Track improvements over time
This is exactly what a test dataset spreadsheet is for.

Test Case ID|Resume Name|JD Name|Experience Level|Fit Score|Matching Skills|Missing Skills|Skill Matrix Summary|Prep Plan Quality (1‚Äì5)|Mock Interview Depth (1‚Äì5)|Resume Rewrite Quality (1‚Äì5)|JSON Output Valid? (Y/N)|Notes / Issues													



‚≠ê Column Explanation (so your team understands it)
Fit Score
From Gemini‚Äôs FitGraph (0‚Äì100).
Matching Skills / Missing Skills
Copy directly from Gemini‚Äôs output.
Skill Matrix Summary
A short summary like:
- Strong in Python fundamentals
- Weak in concurrency
- Needs improvement in SQL
Prep Plan Quality
Rate 1‚Äì5 based on:
- Relevance
- Depth
- Actionability
Mock Interview Depth
Rate 1‚Äì5 based on:
- Follow‚Äëup questions
- Technical depth
- Behavioral coverage
Resume Rewrite Quality
Rate 1‚Äì5 based on:
- Clarity
- Impact
- ATS alignment
JSON Output Valid?
Mark Y/N depending on whether Gemini produced the JSON block correctly.
Notes / Issues
Use this to track:
- Missing JSON
- Wrong structure
- Missing skill matrix
- Tone issues
- Incorrect FitGraph

‚≠ê 3. Why this sheet matters
This becomes your ground truth dataset for:
- Improving prompts
- Measuring Gemini‚Äôs consistency
- Training your team
- Benchmarking new versions
- Debugging LangGraph nodes later
- Evaluating model upgrades (Flash ‚Üí Pro ‚Üí Ultra)
This is how real AI products evolve.

üéØ Prep Plan Quality Scoring Rubric (1‚Äì5 Scale)
This rubric evaluates the clarity, relevance, depth, and actionability of the preparation plan generated by Gemini.

Score | Description
------|---------------------------------------------------------------
1     | Vague, generic advice. No clear steps. No prioritization.
2     | Some relevant skills mentioned, but lacks structure.
3     | Clear steps and relevant skills. Priorities mentioned.
4     | Well-structured plan with prioritized skills. Some personalization.
5     | Highly personalized, deeply relevant. Feels like a mentor‚Äôs guidance.




üß† How to Score Automatically (Python Logic)
You can write a function that checks:
- ‚úÖ Does it include priority levels (high, medium, low)?
- ‚úÖ Are the skills specific and relevant to the JD?
- ‚úÖ Are the steps actionable (e.g., ‚ÄúBuild a REST API using Flask‚Äù)?
- ‚úÖ Is there a sense of progression or timeline?
- ‚úÖ Does it feel personalized to the resume‚Äôs gaps?
Each ‚Äúyes‚Äù adds 1 point to the score.

```Python

def score_prep_plan(prep_plan):
    score = 0
    if "priority" in prep_plan and any(prep_plan["priority"].values()):
        score += 1
    if any("SQL" in step or "Flask" in step for step in prep_plan.get("steps", [])):
        score += 1
    if any("build" in step or "practice" in step for step in prep_plan.get("steps", [])):
        score += 1
    if len(prep_plan.get("steps", [])) >= 3:
        score += 1
    if "confidence" in prep_plan.get("summary", "").lower():
        score += 1
    return min(score, 5)
```

# Metrics Definitions 


‚≠ê Step 1 ‚Äî Define the Metrics You Want to Track
From your table, you already have the raw ingredients.
Here are the 6 performance metrics you can compute over time:
1. Fit Score Stability
- Measures how consistent the FitGraph score is for the same resume + JD.
- Formula:
variance of Fit Score across repeated runs
2. Skill Extraction Accuracy
- Measures how often the model identifies the correct matching/missing skills.
- Formula:
# of correct skills / total expected skills
3. Prep Plan Quality Trend
- Average score over time.
- Formula:
mean(Prep Plan Quality)
4. Mock Interview Depth Trend
- Measures how deep and structured the interview questions are.
- Formula:
mean(Mock Interview Depth)
5. Resume Rewrite Quality Trend
- Measures how well the model rewrites resumes.
- Formula:
mean(Resume Rewrite Quality)
6. JSON Reliability Score
- Measures how often the model returns valid JSON.
- Formula:
# of valid JSON outputs / total runs

‚≠ê Step 2 ‚Äî Add Derived Columns to Your Sheet
Extend your sheet with these new columns:
Consistency Score
Skill Extraction Accuracy
JSON Reliability (1 or 0)
Overall Quality Score


Consistency Score
If you run the same test case multiple times:
- Low variance = high consistency
- High variance = unstable model behavior
Skill Extraction Accuracy
You manually define the ‚Äúexpected skills‚Äù for each JD.
Then compare Gemini‚Äôs output to your expected list.
JSON Reliability
- 1 = JSON valid
- 0 = JSON missing or malformed
Overall Quality Score
Weighted formula (you can tune weights later):
Overall Score = 
  (Fit Score * 0.2) +
  (Prep Plan Quality * 0.2) +
  (Mock Interview Depth * 0.2) +
  (Resume Rewrite Quality * 0.2) +
  (JSON Reliability * 0.2)


This gives you a single number per test case.

‚≠ê Step 3 ‚Äî Use the Data to Evaluate the App Over Time
Once you have 20‚Äì50 test cases, you can compute:
1. Model Stability
- Is the Fit Score fluctuating too much?
- Are skill matrices consistent?
2. Model Quality
- Are prep plans improving?
- Are resume rewrites getting more structured?
3. JSON Reliability
- If JSON reliability is < 0.8 ‚Üí prompt needs refinement.
4. JD‚ÄëSpecific Weaknesses
Example:
- For Data Analyst roles ‚Üí missing Pandas often?
- For Backend roles ‚Üí missing API design often?
5. Resume‚ÄëSpecific Weaknesses
Example:
- For freshers ‚Üí model overestimates experience?
- For mid‚Äëlevel ‚Üí model underestimates depth?

‚≠ê Step 4 ‚Äî Use Metrics to Improve the App
Your metrics will tell you:
If JSON reliability is low ‚Üí tighten system prompt
If Fit Score variance is high ‚Üí lower temperature
If skill extraction accuracy is low ‚Üí add examples
If prep plan quality is low ‚Üí add structure templates
If mock interview depth is inconsistent ‚Üí add interview patterns
This is how you evolve CareerPilot from: ‚ÄúLLM-powered app‚Äù ‚Üí ‚ÄúReliable, production-grade AI system.‚Äù

‚≠ê Step 5 ‚Äî Build a Dashboard (Optional but powerful)
Later, you can build a simple dashboard (Streamlit or Grafana) showing:
- Average Fit Score
- JSON Reliability %
- Prep Plan Quality trend
- Mock Interview Depth trend
- Resume Rewrite Quality trend
- Skill extraction accuracy
- Consistency score
This becomes your internal QA dashboard.

‚≠ê Summary (Copy‚ÄëReady for Your Team)

We will use the test-case data to compute:
- Fit Score Stability
- Skill Extraction Accuracy
- Prep Plan Quality Trend
- Mock Interview Depth Trend
- Resume Rewrite Quality Trend
- JSON Reliability Score
- Overall Quality Score

These metrics will help us:
- Evaluate model consistency
- Identify weak areas
- Improve prompts
- Tune temperature
- Strengthen JSON structure
- Track improvements over time







